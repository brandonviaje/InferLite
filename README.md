# Infera

![C++ CI](https://github.com/brandonviaje/InferLite/actions/workflows/cpp-tests.yml/badge.svg)

lightweight inference engine for executing trained neural network models efficiently

goal: learn the infrastructure behind inference server

A ML inference servver is

## System Architecture

<img width="1000" height="700" alt="Inference Server" src="https://github.com/user-attachments/assets/50655ea3-88e2-40e3-b19d-f4f8e3b80f8a" />

# Acknowledgements

[What is an Inference Server](https://www.doubleword.ai/resources/what-is-an-inference-server-10-characteristics-of-an-effective-inference-server-for-generative-ai-deployments)
[Build Your Own Inference Engine](https://michalpitr.substack.com/p/build-your-own-inference-engine-from)
[Understanding Inference Engine](https://www.gmicloud.ai/glossary/inference-engine)
